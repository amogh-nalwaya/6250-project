"""
    This file contains evaluation methods that take in a set of predicted labels 
        and a set of ground truth labels and calculate precision, recall, accuracy, and f1 score
"""
from collections import defaultdict
import numpy as np

from sklearn.metrics import roc_curve, auc

from constants import *

#def all_metrics(yhat, y, k=8, yhat_raw=None):
def all_metrics(yhat, y, yhat_raw=None):
    """
        Inputs:
            yhat: binary predictions matrix 
            y: binary ground truth matrix
            yhat_raw: prediction scores matrix (floats)
        Outputs:
            dict holding relevant metrics
    """
    names = ["acc", "prec", "rec", "f1"]

    #macro
    macro = all_macro(yhat, y)

    #micro
    ymic = y.ravel() # RAVEL = FLATTEN TO 1D
    yhatmic = yhat.ravel()
    micro = all_micro(yhatmic, ymic)

    metrics = {names[i] + "_macro": macro[i] for i in range(len(macro))}
    metrics.update({names[i] + "_micro": micro[i] for i in range(len(micro))})

    #AUC and @k
    if yhat_raw is not None:
        
        roc_auc = auc_metrics(yhat_raw, y, ymic)
        metrics.update(roc_auc)
        
        ### NOT USING PRECISION@K
        #allow k to be passed as int or list
#        if type(k) != list:
#            k = [k]
#        for k_i in k:
#            rec_at_k = recall_at_k(yhat_raw, y, k_i)
#            metrics['rec_at_%d' % k_i] = rec_at_k
#            prec_at_k = precision_at_k(yhat_raw, y, k_i)
#            metrics['prec_at_%d' % k_i] = prec_at_k
#            metrics['f1_at_%d' % k_i] = 2*(prec_at_k*rec_at_k)/(prec_at_k+rec_at_k)

    return metrics

def all_macro(yhat, y):
    return macro_accuracy(yhat, y), macro_precision(yhat, y), macro_recall(yhat, y), macro_f1(yhat, y)

def all_micro(yhatmic, ymic):
    return micro_accuracy(yhatmic, ymic), micro_precision(yhatmic, ymic), micro_recall(yhatmic, ymic), micro_f1(yhatmic, ymic)

#########################################################################
#MACRO METRICS: calculate metric for each label and average across labels
#########################################################################

def macro_accuracy(yhat, y):
    num = intersect_size(yhat, y, 0) / union_size(yhat, y, 0)
    num[np.isnan(num)] = 0.
    return np.mean(num)

def macro_precision(yhat, y):
    num = intersect_size(yhat, y, 0) / yhat.sum(axis=0)
    #correct for divide-by-zeros
    num[np.isnan(num)] = 0.
    return np.mean(num)

def macro_recall(yhat, y):
    num = intersect_size(yhat, y, 0) / y.sum(axis=0)
    num[np.isnan(num)] = 0.
    return np.mean(num)

def macro_f1(yhat, y):
    prec = macro_precision(yhat, y)
    rec = macro_recall(yhat, y)
    if prec + rec == 0:
        f1 = 0.
    else:
        f1 = 2*(prec*rec)/(prec+rec)
    return f1

###################
# INSTANCE-AVERAGED
###################

def inst_precision(yhat, y):
    num = intersect_size(yhat, y, 1) / yhat.sum(axis=1)
    #correct for divide-by-zeros
    num[np.isnan(num)] = 0.
    return np.mean(num)

def inst_recall(yhat, y):
    num = intersect_size(yhat, y, 1) / y.sum(axis=1)
    #correct for divide-by-zeros
    num[np.isnan(num)] = 0.
    return np.mean(num)

def inst_f1(yhat, y):
    prec = inst_precision(yhat, y)
    rec = inst_recall(yhat, y)
    f1 = 2*(prec*rec)/(prec+rec)
    return f1

##############
# AT-K
##############

#def recall_at_k(yhat_raw, y, k):
#    #num true labels in top k predictions / num true labels
#    sortd = np.argsort(yhat_raw)[:,::-1]
#    topk = sortd[:,:k]
#
#    #get recall at k for each example
#    vals = []
#    for i, tk in enumerate(topk):
#        num_true_in_top_k = y[i,tk].sum()
#        denom = y[i,:].sum()
#        vals.append(num_true_in_top_k / float(denom))
#
#    vals = np.array(vals)
#    vals[np.isnan(vals)] = 0.
#
#    return np.mean(vals)
#
#def precision_at_k(yhat_raw, y, k):
#    #num true labels in top k predictions / num 1 predictions in top k 
#    sortd = np.argsort(yhat_raw)[:,::-1]
#    topk = sortd[:,:k]
#
#    #get precision at k for each example
#    vals = []
#    for i, tk in enumerate(topk):
#        if len(tk) > 0:
#            num_true_in_top_k = y[i,tk].sum()
#            denom = len(tk)
#            vals.append(num_true_in_top_k / float(denom))
#
#    return np.mean(vals)

##########################################################################
#MICRO METRICS: treat every prediction as an individual binary prediction
##########################################################################

def micro_accuracy(yhatmic, ymic):
    return intersect_size(yhatmic, ymic, 0) / union_size(yhatmic, ymic, 0)

def micro_precision(yhatmic, ymic):
    return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)

def micro_recall(yhatmic, ymic):
    return intersect_size(yhatmic, ymic, 0) / ymic.sum(axis=0)

def micro_f1(yhatmic, ymic):
    prec = micro_precision(yhatmic, ymic)
    rec = micro_recall(yhatmic, ymic)
    if prec + rec == 0:
        f1 = 0.
    else:
        f1 = 2*(prec*rec)/(prec+rec)
    return f1

def auc_metrics(yhat_raw, y, ymic):
    if yhat_raw.shape[0] <= 1:
        return
    fpr = {}
    tpr = {}
    roc_auc = {}
    
    ### NOT COMPUTING METRICS FOR INDIVIDUAL LABELS ###
    
#    #get AUC for each label individually
#    relevant_labels = []
#    for i in range(y.shape[1]):
#        #only if there are true positives for this label
#        if y[:,i].sum() > 0:
#            fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])
#            if len(fpr[i]) > 1 and len(tpr[i]) > 1:
#                auc_score = auc(fpr[i], tpr[i])
#                if not np.isnan(auc_score): 
#                    roc_auc["auc_%d" % i] = auc_score
#                    relevant_labels.append(i)


    #macro-AUC: just average the auc scores
    auc_list = []
    for i in range(y.shape[1]): # ASSUMING EACH LABEL CORRESPONDS TO A COLUMN
        #only if there are true positives for this label
        if y[:,i].sum() > 0:
            fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])
            if len(fpr[i]) > 1 and len(tpr[i]) > 1:
                auc_score = auc(fpr[i], tpr[i])
                if not np.isnan(auc_score): 
                    auc_list.append(auc_score)
                else:
                    print("\n*******NAN MACRO AUC SCORE*******\n")
                    
    roc_auc['auc_macro'] = np.mean(auc_list)

    #macro-AUC: just average the auc scores
#    aucs = []
#    for i in relevant_labels:
#        aucs.append(roc_auc['auc_%d' % i])
#    roc_auc['auc_macro'] = np.mean(aucs)


    #micro-AUC: just look at each individual prediction
    yhatmic = yhat_raw.ravel()
    fpr["micro"], tpr["micro"], _ = roc_curve(ymic, yhatmic) 
    roc_auc["auc_micro"] = auc(fpr["micro"], tpr["micro"])

    return roc_auc

def union_size(yhat, y, axis):
    #axis=0 for label-level union (macro). axis=1 for instance-level
    return np.logical_or(yhat, y).sum(axis=axis).astype(float)

def intersect_size(yhat, y, axis):
    #axis=0 for label-level union (macro). axis=1 for instance-level
    return np.logical_and(yhat, y).sum(axis=axis).astype(float)

def print_metrics(metrics):
    #annoyingly complicated printing, to keep track of progress during training
    
#    have_k = False
#    #get k
#    for key in metrics.keys():
#        if key.startswith('rec_at'):
#            have_k = True
#            k = int(key.split('_')[-1])
#        elif key.startswith('prec_at'):
#            k2 = int(key.split('_')[-1])
            
    print
    if "auc_macro" in metrics.keys():
        print("[MACRO] accuracy, precision, recall, f-measure, AUC")
        print("%.4f, %.4f, %.4f, %.4f, %.4f" % (metrics["acc_macro"], metrics["prec_macro"], metrics["rec_macro"], metrics["f1_macro"], metrics["auc_macro"]))
    else:
        print("[MACRO] accuracy, precision, recall, f-measure")
        print("%.4f, %.4f, %.4f, %.4f" % (metrics["acc_macro"], metrics["prec_macro"], metrics["rec_macro"], metrics["f1_macro"]))

    if "auc_micro" in metrics.keys():
        print("[MICRO] accuracy, precision, recall, f-measure, AUC")
        print("%.4f, %.4f, %.4f, %.4f, %.4f" % (metrics["acc_micro"], metrics["prec_micro"], metrics["rec_micro"], metrics["f1_micro"], metrics["auc_micro"]))
    else:
        print("[MICRO] accuracy, precision, recall, f-measure")
        print("%.4f, %.4f, %.4f, %.4f" % (metrics["acc_micro"], metrics["prec_micro"], metrics["rec_micro"], metrics["f1_micro"]))
#    if have_k:
#        print("recall@%d, precision@%d" % (k, k2))
#        print("%.4f, %.4f" % (metrics["rec_at_%d" % k], metrics["prec_at_%d" % k2]))
#    print
