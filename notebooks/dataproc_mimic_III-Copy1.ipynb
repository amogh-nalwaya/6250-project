{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import datasets\n",
    "from learn import models\n",
    "import log_reg\n",
    "from dataproc import extract_wvs\n",
    "from dataproc import get_discharge_summaries\n",
    "from dataproc import concat_and_split\n",
    "from dataproc import build_vocab\n",
    "from dataproc import vocab_index_descriptions\n",
    "from dataproc import word_embeddings\n",
    "from constants import MIMIC_3_DIR, DATA_DIR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import csv\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some data processing in a much better way, with a notebook.\n",
    "\n",
    "First, let's define some stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = 'full' #use all available labels in the dataset for prediction\n",
    "notes_file = '%s/NOTEEVENTS.csv' % MIMIC_3_DIR # raw note events downloaded from MIMIC-III\n",
    "vocab_size = 'full' #don't limit the vocab size to a specific number\n",
    "vocab_min = 3 #discard tokens appearing in fewer than this many documents\n",
    "split = [.9, .1/3, .2/3] #train/dev/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine diagnosis and procedure codes and reformat them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes in MIMIC-III are given in separate files for procedures and diagnoses, and the codes are given without periods, which might lead to collisions if we naively combine them. So we have to add the periods back in the right place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many codes are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and preprocess raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing time!\n",
    "\n",
    "This will:\n",
    "- Select only discharge summaries and their addenda\n",
    "- remove punctuation and numeric-only tokens, removing 500 but keeping 250mg\n",
    "- lowercase all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:00, 1177.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing notes file\n",
      "writing to ../mimicdata/mimic3/disch_full.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2083180it [01:20, 26021.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#This reads all notes, selects only the discharge summaries, and tokenizes them, returning the output filename\n",
    "disch_full_file = get_discharge_summaries.write_discharge_summaries(out_file=\"%s/disch_full.csv\" % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read this in and see what kind of data we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('%s/disch_full.csv' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUBJECT_ID  HADM_ID  CHARTTIME  \\\n",
      "0       22532   167853        NaN   \n",
      "1       13702   107527        NaN   \n",
      "2       13702   167118        NaN   \n",
      "3       13702   196489        NaN   \n",
      "4       26880   135453        NaN   \n",
      "\n",
      "                                                TEXT  \n",
      "0  admission date discharge date service addendum...  \n",
      "1  admission date discharge date date of birth se...  \n",
      "2  admission date discharge date service cardioth...  \n",
      "3  admission date discharge date service medicine...  \n",
      "4  admission date discharge date date of birth se...  \n",
      "59652\n",
      "admission date discharge date date of birth sex m service cardiac surgery chief complaint chest pain vessel disease on catheterization history of present illness the patient is a year old male transferred from hospital6 to the hospital1 status post catheterization revealing vessel cardiac disease the patient presented to hospital6 with gradually increasing chest pain over the past three to four months to the point that he had chest pain with minimal exertion past medical history known coronary artery disease status post catheterization years ago at hospital1 heavy smoker hypertension gastroesophageal reflux disease peptic ulcer disease wegener granulomatosis with complete resolution glaucoma past surgical history perforated ulcer medications on admission lisinopril mg p o q d prilosec mg p o q d cosopt eyedrops alphagan eyedrops travatan eyedrops lansoprazole mg p o q d allergies no known drug allergies hospital course the patient underwent an elective coronary artery bypass graft times three on with grafts being a left internal mammary artery to left anterior descending artery saphenous vein graft to ramus and saphenous vein graft to posterior descending artery he was extubated on the day of surgery on postoperative day one his nasogastric tubes were discontinued he was transferred to the regular floor on postoperative day one he subsequently had a smooth postoperative course his pacing wires were discontinued on postoperative day three by postoperative day he was ambulating well he was comfortable on p o pain medication and he was ready for discharge home medications on discharge lasix mg p o q d for one week kcl meq p o q d for one week colace mg p o b i d zantac mg p o b i d enteric coated aspirin mg p o q d alphagan eyedrops lopressor mg p o b i d nicoderm patch mg q d percocet one to two tablets p o q 6h p r n di last name stitle e followup follow up with primary care physician last name namepattern4 first name4 namepattern1 last name namepattern1 in two weeks and with dr last name prefixes in four weeks condition at discharge condition on discharge was stable discharge status discharged to home doctor last name last name prefixes m d md number dictated by last name namepattern1 medquist36 d t job job number\n",
      "admission date discharge date date of birth sex m service cardiac surgery chief complaint chest pain vessel disease on catheterization history of present illness the patient is a year old male transferred from hospital6 to the hospital1 status post catheterization revealing vessel cardiac disease the patient presented to hospital6 with gradually increasing chest pain over the past three to four months to the point that he had chest pain with minimal exertion past medical history known coronary artery disease status post catheterization years ago at hospital1 heavy smoker hypertension gastroesophageal reflux disease peptic ulcer disease wegener granulomatosis with complete resolution glaucoma past surgical history perforated ulcer medications on admission lisinopril mg p o q d prilosec mg p o q d cosopt eyedrops alphagan eyedrops travatan eyedrops lansoprazole mg p o q d allergies no known drug allergies hospital course the patient underwent an elective coronary artery bypass graft times three on with grafts being a left internal mammary artery to left anterior descending artery saphenous vein graft to ramus and saphenous vein graft to posterior descending artery he was extubated on the day of surgery on postoperative day one his nasogastric tubes were discontinued he was transferred to the regular floor on postoperative day one he subsequently had a smooth postoperative course his pacing wires were discontinued on postoperative day three by postoperative day he was ambulating well he was comfortable on p o pain medication and he was ready for discharge home medications on discharge lasix mg p o q d for one week kcl meq p o q d for one week colace mg p o b i d zantac mg p o b i d enteric coated aspirin mg p o q d alphagan eyedrops lopressor mg p o b i d nicoderm patch mg q d percocet one to two tablets p o q 6h p r n di last name stitle e followup follow up with primary care physician last name namepattern4 first name4 namepattern1 last name namepattern1 in two weeks and with dr last name prefixes in four weeks condition at discharge condition on discharge was stable discharge status discharged to home doctor last name last name prefixes m d md number dictated by last name namepattern1 medquist36 d t job job number\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(len(df.index))\n",
    "df[df.duplicated(subset=['HADM_ID'])].head()\n",
    "#print(df[df['HADM_ID'== 169684]])\n",
    "new_df = df[df['HADM_ID'] == 169684]\n",
    "for index, row in new_df.iterrows():\n",
    "    print (row['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52726"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many admissions?\n",
    "len(df['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokens and types\n",
    "types = set()\n",
    "num_tok = 0\n",
    "for row in df.itertuples():\n",
    "    for w in row[4].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num types 150854\n",
      "Num tokens 79801387\n"
     ]
    }
   ],
   "source": [
    "print(\"Num types\", len(types))\n",
    "print(\"Num tokens\", str(num_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's sort by SUBJECT_ID and HADM_ID to make a correspondence with the MIMIC-3 label file\n",
    "df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate labels with set of discharge summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there were some HADM_ID's that didn't have discharge summaries, so they weren't included with our notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append labels to notes in a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's append each instance with all of its codes\n",
    "#this is pretty non-trivial so let's use this script I wrote, which requires the notes to be written to file\n",
    "sorted_file = '%s/disch_full.csv' % MIMIC_3_DIR\n",
    "df.to_csv(sorted_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/dev/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SUBJECT_ID  HADM_ID  CHARTTIME  \\\n",
      "48470           3   145834        NaN   \n",
      "4782            4   185777        NaN   \n",
      "24476           6   107064        NaN   \n",
      "22764           9   150750        NaN   \n",
      "57328           9   150750        NaN   \n",
      "\n",
      "                                                    TEXT  \n",
      "48470  admission date discharge date date of birth se...  \n",
      "4782   admission date discharge date date of birth se...  \n",
      "24476  admission date discharge date date of birth se...  \n",
      "22764  admission date discharge date date of birth se...  \n",
      "57328  name known lastname known firstname unit no nu...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(df))\n",
    "#print(df[~(df.duplicated(subset=['SUBJECT_ID','HADM_ID', 'TEXT'])) * df.duplicated(subset=['SUBJECT_ID','HADM_ID'])])\n",
    "#len(df[~df.duplicated(subset=['SUBJECT_ID','HADM_ID']) - df.duplicated(subset=['SUBJECT_ID','HADM_ID', 'TEXT'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data...\n",
      "removing rare terms\n",
      "55418 terms qualify out of 150854 total\n",
      "writing output\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(build_vocab)\n",
    "vocab_min = 3\n",
    "vname = '%s/vocab.csv' % MIMIC_3_DIR\n",
    "infile = '%s/disch_full.csv' % MIMIC_3_DIR\n",
    "build_vocab.build_vocab(vocab_min, infile, vname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sort each data split by length for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/disch_%s_split.csv' % (MIMIC_3_DIR, splt)\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_full.csv' % (MIMIC_3_DIR, splt), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write BOW files for input to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = 'full'\n",
    "ind2w, w2ind, ind2c, c2ind, _, _ = datasets.load_lookups('%s/train_%s.csv' % (MIMIC_3_DIR, str(Y)),\n",
    "                                                         '%s/vocab.csv' % MIMIC_3_DIR, \n",
    "                                                         Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, yy, hadm_ids = log_reg.construct_X_Y('%s/train_%s.csv' % (MIMIC_3_DIR, str(Y)), Y, w2ind, c2ind, 'mimic3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.write_bows('%s/train_%s.csv' % (MIMIC_3_DIR, str(Y)), X, hadm_ids, yy, ind2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, yy, hadm_ids = log_reg.construct_X_Y('%s/test_%s.csv' % (MIMIC_3_DIR, str(Y)), Y, w2ind, c2ind, 'mimic3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.write_bows('%s/test_%s.csv' % (MIMIC_3_DIR, str(Y)), X, hadm_ids, yy, ind2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train word embeddings on all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building word2vec vocab on ../mimicdata/mimic3/disch_full.csv...\n",
      "training...\n",
      "writing embeddings to ../mimicdata/mimic3/processed_full.w2v\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(word_embeddings)\n",
    "w2v_file = word_embeddings.word_embeddings('full', '%s/disch_full.csv' % MIMIC_3_DIR, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pre-trained word embeddings with new vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55418/55418 [02:02<00:00, 453.64it/s] \n"
     ]
    }
   ],
   "source": [
    "importlib.reload(extract_wvs)\n",
    "#import importlib\n",
    "importlib.reload(datasets)\n",
    "Y = 'full'\n",
    "extract_wvs.gensim_to_embeddings('%s/processed_full.w2v' % MIMIC_3_DIR, '%s/vocab.csv' % MIMIC_3_DIR, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process code descriptions using the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(vocab_index_descriptions)\n",
    "reload(datasets)\n",
    "vocab_index_descriptions.vocab_index_descriptions('%s/vocab.csv' % MIMIC_3_DIR,\n",
    "                                                  '%s/description_vectors.vocab' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter each split to the top 50 diagnosis/procedure codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first calculate the top k\n",
    "counts = Counter()\n",
    "dfnl = pd.read_csv('%s/notes_labeled.csv' % MIMIC_3_DIR)\n",
    "for row in dfnl.itertuples():\n",
    "    for label in str(row[4]).split(';'):\n",
    "        counts[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codes_50 = [code[0] for code in codes_50[:Y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "codes_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('%s/TOP_%s_CODES.csv' % (MIMIC_3_DIR, str(Y)), 'w') as of:\n",
    "    w = csv.writer(of)\n",
    "    for code in codes_50:\n",
    "        w.writerow([code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    print(splt)\n",
    "    hadm_ids = set()\n",
    "    with open('%s/%s_50_hadm_ids.csv' % (MIMIC_3_DIR, splt), 'r') as f:\n",
    "        for line in f:\n",
    "            hadm_ids.add(line.rstrip())\n",
    "    with open('%s/notes_labeled.csv' % MIMIC_3_DIR, 'r') as f:\n",
    "        with open('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), 'w') as of:\n",
    "            r = csv.reader(f)\n",
    "            w = csv.writer(of)\n",
    "            #header\n",
    "            w.writerow(next(r))\n",
    "            i = 0\n",
    "            for row in r:\n",
    "                hadm_id = row[1]\n",
    "                if hadm_id not in hadm_ids:\n",
    "                    continue\n",
    "                codes = set(str(row[3]).split(';'))\n",
    "                filtered_codes = codes.intersection(set(codes_50))\n",
    "                if len(filtered_codes) > 0:\n",
    "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y))\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write BOW files for input to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = 50\n",
    "ind2w, w2ind, ind2c, c2ind, _, _ = datasets.load_lookups('%s/train_%s.csv' % (MIMIC_3_DIR, str(Y)),\n",
    "                                                         '%s/vocab.csv' % MIMIC_3_DIR, \n",
    "                                                         Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, yy, hadm_ids = log_reg.construct_X_Y('%s/train_%s.csv' % (MIMIC_3_DIR, str(Y)), Y, w2ind, c2ind, 'mimic3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.write_bows('%s/train_%s.csv' % (MIMIC_3_DIR, str(Y)), X, hadm_ids, yy, ind2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, yy, hadm_ids = log_reg.construct_X_Y('%s/test_%s.csv' % (MIMIC_3_DIR, str(Y)), Y, w2ind, c2ind, 'mimic3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.write_bows('%s/test_%s.csv' % (MIMIC_3_DIR, str(Y)), X, hadm_ids, yy, ind2c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
